"""
Stage 1: SFT to prime the model on reasoning format.

This teaches Vocalino to output reasoning text before audio tokens.
Run this BEFORE stage2_grpo.py.

First run generate_reasoning_data.py to create the reasoning dataset,
then run this script to train.

The model learns:
<START_OF_HUMAN><start_of_caption>{caption}<end_of_caption>{text}<END_OF_TEXT><END_OF_HUMAN>
<START_OF_AI><start_of_reasoning>{reasoning}<end_of_reasoning><START_OF_SPEECH>{audio_tokens}<END_OF_SPEECH><END_OF_AI>
"""

import torch
import torchaudio.transforms as T
from datasets import Audio, load_dataset, Dataset
from snac import SNAC
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
import os
import json

# =============================================================================
# CONFIG
# =============================================================================
MODEL_NAME = "ChristophSchuhmann/Vocalino_0.11_alpha"
REASONING_DATA_PATH = "reasoning_sft_data.json"  # Generated by generate_reasoning_data.py
AUDIO_DATASET = "mrfakename/voice-acting"  # Original dataset with audio
OUTPUT_DIR = "outputs_reasoning_sft"
DEVICE = "cuda"
SAMPLE_RATE = 24000
MAX_SEQ_LENGTH = 4096

# =============================================================================
# SPECIAL TOKENS
# =============================================================================
TOKEN_OFFSET_BASE = 128266
LAYER_OFFSETS = [0, 4096, 8192, 12288, 16384, 20480, 24576]

START_OF_SPEECH = 128257
END_OF_SPEECH = 128258
START_OF_HUMAN = 128259
END_OF_HUMAN = 128260
START_OF_AI = 128261
END_OF_AI = 128262
END_OF_TEXT = 128009

# Reasoning markers (text-based, tokenizer will encode them)
REASONING_START = "<start_of_reasoning>"
REASONING_END = "<end_of_reasoning>"

# =============================================================================
# LOAD MODEL
# =============================================================================
print("Loading model...")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.bfloat16,
    device_map="auto",
)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Ensure tokenizer has proper special tokens
if tokenizer.eos_token_id is None:
    tokenizer.eos_token_id = END_OF_TEXT
if tokenizer.pad_token_id is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id

# Get token IDs for reasoning markers
REASONING_START_IDS = tokenizer.encode(REASONING_START, add_special_tokens=False)
REASONING_END_IDS = tokenizer.encode(REASONING_END, add_special_tokens=False)

# =============================================================================
# LOAD SNAC
# =============================================================================
print("Loading SNAC...")
snac_model = SNAC.from_pretrained("hubertsiuzdak/snac_24khz").to(DEVICE)

# =============================================================================
# AUDIO TOKENIZATION
# =============================================================================
def tokenize_audio(waveform, orig_sr):
    """Convert audio waveform to SNAC tokens."""
    waveform = torch.from_numpy(waveform).unsqueeze(0).to(dtype=torch.float32)

    # Resample to 24kHz if needed
    if orig_sr != SAMPLE_RATE:
        resampler = T.Resample(orig_freq=orig_sr, new_freq=SAMPLE_RATE)
        waveform = resampler(waveform)

    waveform = waveform.unsqueeze(0).to(DEVICE)

    with torch.inference_mode():
        codes = snac_model.encode(waveform)

    all_codes = []
    for i in range(codes[0].shape[1]):
        all_codes.append(codes[0][0][i].item() + TOKEN_OFFSET_BASE + LAYER_OFFSETS[0])
        all_codes.append(codes[1][0][2 * i].item() + TOKEN_OFFSET_BASE + LAYER_OFFSETS[1])
        all_codes.append(codes[2][0][4 * i].item() + TOKEN_OFFSET_BASE + LAYER_OFFSETS[2])
        all_codes.append(codes[2][0][(4 * i) + 1].item() + TOKEN_OFFSET_BASE + LAYER_OFFSETS[3])
        all_codes.append(codes[1][0][(2 * i) + 1].item() + TOKEN_OFFSET_BASE + LAYER_OFFSETS[4])
        all_codes.append(codes[2][0][(4 * i) + 2].item() + TOKEN_OFFSET_BASE + LAYER_OFFSETS[5])
        all_codes.append(codes[2][0][(4 * i) + 3].item() + TOKEN_OFFSET_BASE + LAYER_OFFSETS[6])

    return all_codes

# =============================================================================
# DATASET
# =============================================================================
print("Loading reasoning data...")
with open(REASONING_DATA_PATH, "r") as f:
    reasoning_data = json.load(f)

# Create lookup by (caption, text) -> reasoning
reasoning_lookup = {
    (item["caption"], item["text"]): item["reasoning"]
    for item in reasoning_data
}
print(f"Loaded {len(reasoning_lookup)} reasoning examples")

# Load audio dataset
print("Loading audio dataset...")
audio_dataset = load_dataset(AUDIO_DATASET, split="train")
audio_dataset = audio_dataset.cast_column("audio", Audio(sampling_rate=SAMPLE_RATE))

# Filter to only examples we have reasoning for
def has_reasoning(example):
    return (example.get("caption", ""), example.get("text", "")) in reasoning_lookup

audio_dataset = audio_dataset.filter(has_reasoning)
print(f"Found {len(audio_dataset)} examples with matching reasoning")

def process_example(example):
    """
    Process a single example into the full sequence format.
    Pairs reasoning from generated data with audio from original dataset.
    """
    try:
        audio_data = example.get("audio")
        if not audio_data or "array" not in audio_data:
            return {"input_ids": None, "labels": None, "attention_mask": None}

        # Tokenize audio
        audio_tokens = tokenize_audio(audio_data["array"], audio_data["sampling_rate"])

        if not audio_tokens:
            return {"input_ids": None, "labels": None, "attention_mask": None}

        caption = example.get("caption", "")
        text = example.get("text", "")
        reasoning = reasoning_lookup.get((caption, text), "")

        # Build text content: <start_of_caption>{caption}<end_of_caption>{text}
        text_content = f"<start_of_caption>{caption}<end_of_caption>{text}"
        text_ids = tokenizer.encode(text_content, add_special_tokens=False)

        # Build reasoning content
        reasoning_content = f"{REASONING_START}{reasoning}{REASONING_END}"
        reasoning_ids = tokenizer.encode(reasoning_content, add_special_tokens=False)

        # Full sequence:
        # <START_OF_HUMAN> + text + <END_OF_TEXT> + <END_OF_HUMAN>
        # <START_OF_AI> + reasoning + <START_OF_SPEECH> + audio + <END_OF_SPEECH> + <END_OF_AI>
        input_ids = (
            [START_OF_HUMAN]
            + text_ids
            + [END_OF_TEXT, END_OF_HUMAN]
            + [START_OF_AI]
            + reasoning_ids
            + [START_OF_SPEECH]
            + audio_tokens
            + [END_OF_SPEECH, END_OF_AI]
        )

        # Truncate if too long
        if len(input_ids) > MAX_SEQ_LENGTH:
            input_ids = input_ids[:MAX_SEQ_LENGTH]

        return {
            "input_ids": input_ids,
            "labels": input_ids,
            "attention_mask": [1] * len(input_ids),
        }

    except Exception as e:
        print(f"Error processing example: {e}")
        return {"input_ids": None, "labels": None, "attention_mask": None}

print("Processing dataset...")
dataset = audio_dataset.map(process_example, remove_columns=audio_dataset.column_names)

# Filter out failed examples
dataset = dataset.filter(lambda x: x["input_ids"] is not None)

print(f"Dataset size: {len(dataset)}")

if len(dataset) == 0:
    raise ValueError("Dataset is empty after processing! Check your data format.")

# =============================================================================
# DATA COLLATOR
# =============================================================================
from dataclasses import dataclass
from typing import Any, Dict, List

@dataclass
class DataCollatorForSeq2Seq:
    tokenizer: Any
    padding: bool = True
    max_length: int = MAX_SEQ_LENGTH

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        input_ids = [f["input_ids"] for f in features]
        labels = [f["labels"] for f in features]
        attention_mask = [f["attention_mask"] for f in features]

        # Pad to max length in batch
        max_len = min(max(len(x) for x in input_ids), self.max_length)

        padded_input_ids = []
        padded_labels = []
        padded_attention_mask = []

        for inp, lab, att in zip(input_ids, labels, attention_mask):
            pad_len = max_len - len(inp)
            padded_input_ids.append(inp + [self.tokenizer.pad_token_id] * pad_len)
            padded_labels.append(lab + [-100] * pad_len)  # -100 = ignore in loss
            padded_attention_mask.append(att + [0] * pad_len)

        return {
            "input_ids": torch.tensor(padded_input_ids, dtype=torch.long),
            "labels": torch.tensor(padded_labels, dtype=torch.long),
            "attention_mask": torch.tensor(padded_attention_mask, dtype=torch.long),
        }

# =============================================================================
# TRAINING
# =============================================================================
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    num_train_epochs=2,  # Short SFT, just to learn the format
    learning_rate=2e-5,
    warmup_steps=50,
    logging_steps=10,
    save_steps=100,
    save_total_limit=3,
    bf16=True,
    lr_scheduler_type="cosine",
    optim="adamw_torch",
    report_to="wandb",
    dataloader_num_workers=4,
    run_name="vocalino_reasoning_sft",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    data_collator=DataCollatorForSeq2Seq(tokenizer),
)

# =============================================================================
# TRAIN
# =============================================================================
if __name__ == "__main__":
    print("Starting SFT training...")
    print("This will teach the model to output reasoning before audio.")
    trainer.train()

    print("Saving model...")
    model.save_pretrained(f"{OUTPUT_DIR}/final")
    tokenizer.save_pretrained(f"{OUTPUT_DIR}/final")
    print(f"Done! Model saved to {OUTPUT_DIR}/final")
    print("Now run stage2_grpo.py to improve with reinforcement learning.")
