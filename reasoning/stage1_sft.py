"""
Stage 1: SFT to prime the model on reasoning format.

This teaches Vocalino to output reasoning text before audio tokens.
Run this BEFORE stage2_grpo.py.

First run generate_reasoning_data.py to create the reasoning dataset,
then run this script to train.

The model learns:
{voice}: <start_of_caption>{caption}<end_of_caption><start_of_reasoning>{reasoning}<end_of_reasoning>{text}
-> <audio_tokens>
"""

import torch
import torchaudio.transforms as T
from datasets import Audio, load_dataset, Dataset
from snac import SNAC
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
import os
import json

# =============================================================================
# CONFIG
# =============================================================================
MODEL_NAME = "mrfakename/Vocalino-GRPO"
REASONING_DATA_PATH = "reasoning_sft_data.json"  # Generated by generate_reasoning_data.py
AUDIO_DATASET = "mrfakename/voice-acting"  # Original dataset with audio
OUTPUT_DIR = "outputs_reasoning_sft"
DEVICE = "cuda"
SAMPLE_RATE = 24000
MAX_SEQ_LENGTH = 4096

# =============================================================================
# SPECIAL TOKENS
# =============================================================================
TOKEN_OFFSET_BASE = 128266
LAYER_OFFSETS = [0, 4096, 8192, 12288, 16384, 20480, 24576]

# Reasoning markers (text-based, tokenizer will encode them)
REASONING_START = "<start_of_reasoning>"
REASONING_END = "<end_of_reasoning>"

# =============================================================================
# LOAD MODEL
# =============================================================================
print("Loading model...")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.bfloat16,
    device_map="auto",
)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Ensure tokenizer has pad token
if tokenizer.pad_token_id is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id

# =============================================================================
# LOAD SNAC
# =============================================================================
print("Loading SNAC...")
snac_model = SNAC.from_pretrained("hubertsiuzdak/snac_24khz").to(DEVICE)

# =============================================================================
# AUDIO TOKENIZATION
# =============================================================================
def tokenize_audio(waveform, orig_sr):
    """Convert audio waveform to SNAC tokens."""
    waveform = torch.from_numpy(waveform).unsqueeze(0).to(dtype=torch.float32)

    # Resample to 24kHz if needed
    if orig_sr != SAMPLE_RATE:
        resampler = T.Resample(orig_freq=orig_sr, new_freq=SAMPLE_RATE)
        waveform = resampler(waveform)

    waveform = waveform.unsqueeze(0).to(DEVICE)

    with torch.inference_mode():
        codes = snac_model.encode(waveform)

    all_codes = []
    for i in range(codes[0].shape[1]):
        all_codes.append(codes[0][0][i].item() + TOKEN_OFFSET_BASE + LAYER_OFFSETS[0])
        all_codes.append(codes[1][0][2 * i].item() + TOKEN_OFFSET_BASE + LAYER_OFFSETS[1])
        all_codes.append(codes[2][0][4 * i].item() + TOKEN_OFFSET_BASE + LAYER_OFFSETS[2])
        all_codes.append(codes[2][0][(4 * i) + 1].item() + TOKEN_OFFSET_BASE + LAYER_OFFSETS[3])
        all_codes.append(codes[1][0][(2 * i) + 1].item() + TOKEN_OFFSET_BASE + LAYER_OFFSETS[4])
        all_codes.append(codes[2][0][(4 * i) + 2].item() + TOKEN_OFFSET_BASE + LAYER_OFFSETS[5])
        all_codes.append(codes[2][0][(4 * i) + 3].item() + TOKEN_OFFSET_BASE + LAYER_OFFSETS[6])

    return all_codes

# =============================================================================
# DATASET
# =============================================================================
print("Loading reasoning data...")
with open(REASONING_DATA_PATH, "r") as f:
    reasoning_data = json.load(f)

# Create lookup by (caption, text) -> (reasoning, voice)
reasoning_lookup = {
    (item["caption"], item["text"]): (item["reasoning"], item.get("voice", "tara"))
    for item in reasoning_data
}
print(f"Loaded {len(reasoning_lookup)} reasoning examples")

# Load audio dataset
print("Loading audio dataset...")
audio_dataset = load_dataset(AUDIO_DATASET, split="train")
audio_dataset = audio_dataset.cast_column("audio", Audio(sampling_rate=SAMPLE_RATE))

# Filter to only examples we have reasoning for
def has_reasoning(example):
    return (example.get("caption", ""), example.get("text", "")) in reasoning_lookup

audio_dataset = audio_dataset.filter(has_reasoning)
print(f"Found {len(audio_dataset)} examples with matching reasoning")

def process_example(example):
    """
    Process a single example into the full sequence format.
    Uses the same format as inference.py:

    Input (user message): {voice}: <start_of_caption>{caption}<end_of_caption><start_of_reasoning>{reasoning}<end_of_reasoning>{text}
    Output (assistant): <audio_tokens>
    """
    try:
        audio_data = example.get("audio")
        if not audio_data or "array" not in audio_data:
            return {"input_ids": None, "labels": None, "attention_mask": None}

        # Tokenize audio
        audio_tokens = tokenize_audio(audio_data["array"], audio_data["sampling_rate"])

        if not audio_tokens:
            return {"input_ids": None, "labels": None, "attention_mask": None}

        caption = example.get("caption", "")
        text = example.get("text", "")
        reasoning, voice = reasoning_lookup.get((caption, text), ("", "tara"))

        # Build prompt in the same format as inference.py
        # {voice}: <start_of_caption>{caption}<end_of_caption><start_of_reasoning>{reasoning}<end_of_reasoning>{text}
        prompt = f"{voice}: <start_of_caption>{caption}<end_of_caption>{REASONING_START}{reasoning}{REASONING_END}{text}"

        # Use chat template to get input_ids (matches inference.py)
        messages = [{"role": "user", "content": prompt}]

        # Get the prompt tokens using chat template
        prompt_ids = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=True,
        )

        # Full sequence: prompt + audio tokens
        input_ids = prompt_ids + audio_tokens

        # Truncate if too long
        if len(input_ids) > MAX_SEQ_LENGTH:
            input_ids = input_ids[:MAX_SEQ_LENGTH]

        # Labels: mask the prompt, only train on audio tokens
        # -100 means ignore in loss computation
        labels = [-100] * len(prompt_ids) + audio_tokens
        if len(labels) > MAX_SEQ_LENGTH:
            labels = labels[:MAX_SEQ_LENGTH]

        return {
            "input_ids": input_ids,
            "labels": labels,
            "attention_mask": [1] * len(input_ids),
        }

    except Exception as e:
        print(f"Error processing example: {e}")
        return {"input_ids": None, "labels": None, "attention_mask": None}

print("Processing dataset...")
dataset = audio_dataset.map(process_example, remove_columns=audio_dataset.column_names)

# Filter out failed examples
dataset = dataset.filter(lambda x: x["input_ids"] is not None)

print(f"Dataset size: {len(dataset)}")

if len(dataset) == 0:
    raise ValueError("Dataset is empty after processing! Check your data format.")

# =============================================================================
# DATA COLLATOR
# =============================================================================
from dataclasses import dataclass
from typing import Any, Dict, List

@dataclass
class DataCollatorForSeq2Seq:
    tokenizer: Any
    padding: bool = True
    max_length: int = MAX_SEQ_LENGTH

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        input_ids = [f["input_ids"] for f in features]
        labels = [f["labels"] for f in features]
        attention_mask = [f["attention_mask"] for f in features]

        # Pad to max length in batch
        max_len = min(max(len(x) for x in input_ids), self.max_length)

        padded_input_ids = []
        padded_labels = []
        padded_attention_mask = []

        for inp, lab, att in zip(input_ids, labels, attention_mask):
            pad_len = max_len - len(inp)
            padded_input_ids.append(inp + [self.tokenizer.pad_token_id] * pad_len)
            padded_labels.append(lab + [-100] * pad_len)  # -100 = ignore in loss
            padded_attention_mask.append(att + [0] * pad_len)

        return {
            "input_ids": torch.tensor(padded_input_ids, dtype=torch.long),
            "labels": torch.tensor(padded_labels, dtype=torch.long),
            "attention_mask": torch.tensor(padded_attention_mask, dtype=torch.long),
        }

# =============================================================================
# TRAINING
# =============================================================================
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    num_train_epochs=2,  # Short SFT, just to learn the format
    learning_rate=2e-5,
    warmup_steps=50,
    logging_steps=10,
    save_steps=100,
    save_total_limit=3,
    bf16=True,
    lr_scheduler_type="cosine",
    optim="adamw_torch",
    report_to="wandb",
    dataloader_num_workers=4,
    run_name="vocalino_reasoning_sft",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    data_collator=DataCollatorForSeq2Seq(tokenizer),
)

# =============================================================================
# TRAIN
# =============================================================================
if __name__ == "__main__":
    print("Starting SFT training...")
    print("This will teach the model to output reasoning before audio.")
    trainer.train()

    print("Saving model...")
    model.save_pretrained(f"{OUTPUT_DIR}/final")
    tokenizer.save_pretrained(f"{OUTPUT_DIR}/final")
    print(f"Done! Model saved to {OUTPUT_DIR}/final")
    print("Now run stage2_grpo.py to improve with reinforcement learning.")
