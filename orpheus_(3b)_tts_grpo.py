# -*- coding: utf-8 -*-
"""Orpheus_(3B)-TTS_GRPO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/Etherll/notebooks/blob/Orpheus-TTS-GRPO/nb/Orpheus_(3B)-TTS_GRPO.ipynb
"""

import os
if "COLAB_" not in "".join(os.environ.keys()):
    !pip install unsloth vllm
else:
    # Do this only in Colab notebooks! Otherwise use pip install unsloth
    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo
    !pip install sentencepiece protobuf "datasets>=3.4.1" huggingface_hub hf_transfer
    !pip install transformers==4.51.3
    !pip install --no-deps unsloth vllm==0.8.5.post1
!pip install snac
!pip install msgspec blake3 fastapi gguf
pip install wandb
!pip install soundfile librosa evaluate jiwer
!pip install git+https://github.com/wenet-e2e/wespeaker.git
!pip install cloudpickle openai py-cpuinfo
!pip install llguidance xgrammar
!pip install numpy==1.24
!pip install torchmetrics[audio]
!pip install audiobox_aesthetics

!sudo apt install build-essential

!pip install xformers==0.0.29.post3 cachetools

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %autoreload 2

from unsloth import FastLanguageModel
import torch

fourbit_models = [
    # 4bit dynamic quants for superior accuracy and low memory use
    "unsloth/gemma-3-4b-it-unsloth-bnb-4bit",
    "unsloth/gemma-3-12b-it-unsloth-bnb-4bit",
    "unsloth/gemma-3-27b-it-unsloth-bnb-4bit",
    # Qwen3 new models
    "unsloth/Qwen3-4B-unsloth-bnb-4bit",
    "unsloth/Qwen3-8B-unsloth-bnb-4bit",
    # Other very popular models!
    "unsloth/Llama-3.1-8B",
    "unsloth/Llama-3.2-3B",
    "unsloth/Llama-3.3-70B",
    "unsloth/mistral-7b-instruct-v0.3",
    "unsloth/Phi-4",
]  # More models at https://huggingface.co/unsloth
# We need to train LoRA weights first you can use the unsloth LoRA training notebook
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="Etherll/Elise-lora",
    max_seq_length=2048,  # Choose any for long context!
    dtype=None,  # Select None for auto detection
    load_in_4bit=False,  # Select True for 4bit which reduces memory usage
    fast_inference=True,  # Enable vLLM fast inference
    max_lora_rank=64,
    # token="",
    gpu_memory_utilization=0.8,  # Reduce if out of memory)
)

from datasets import Audio,load_dataset,load_from_disk

dataset = load_dataset("MrDragonFox/Elise",split="train")
dataset = dataset.cast_column("audio", Audio(sampling_rate=24000))

# @title Tokenization Function

import locale
import torchaudio.transforms as T
import os
import torch
from snac import SNAC

locale.getpreferredencoding = lambda: "UTF-8"
ds_sample_rate = dataset[0]["audio"]["sampling_rate"]

snac_model = SNAC.from_pretrained("hubertsiuzdak/snac_24khz")
snac_model = snac_model.to("cuda")


def tokenise_audio(waveform):
    waveform = torch.from_numpy(waveform).unsqueeze(0)
    waveform = waveform.to(dtype=torch.float32)
    resample_transform = T.Resample(orig_freq=ds_sample_rate, new_freq=24000)
    waveform = resample_transform(waveform)

    waveform = waveform.unsqueeze(0).to("cuda")

    # generate the codes from snac
    with torch.inference_mode():
        codes = snac_model.encode(waveform)

    all_codes = []
    for i in range(codes[0].shape[1]):
        all_codes.append(codes[0][0][i].item() + 128266)
        all_codes.append(codes[1][0][2 * i].item() + 128266 + 4096)
        all_codes.append(codes[2][0][4 * i].item() + 128266 + (2 * 4096))
        all_codes.append(codes[2][0][(4 * i) + 1].item() + 128266 + (3 * 4096))
        all_codes.append(codes[1][0][(2 * i) + 1].item() + 128266 + (4 * 4096))
        all_codes.append(codes[2][0][(4 * i) + 2].item() + 128266 + (5 * 4096))
        all_codes.append(codes[2][0][(4 * i) + 3].item() + 128266 + (6 * 4096))

    return all_codes


def add_codes(example):
    # Always initialize codes_list to None
    codes_list = None

    try:
        answer_audio = example.get("audio")
        # If there's a valid audio array, tokenise it
        if answer_audio and "array" in answer_audio:
            audio_array = answer_audio["array"]
            codes_list = tokenise_audio(audio_array)
    except Exception as e:
        print(f"Skipping row due to error: {e}")
        # Keep codes_list as None if we fail
    example["codes_list"] = codes_list

    return example


tokeniser_length = 128256
start_of_text = 128000
end_of_text = 128009

start_of_speech = tokeniser_length + 1
end_of_speech = tokeniser_length + 2

start_of_human = tokeniser_length + 3
end_of_human = tokeniser_length + 4

start_of_ai = tokeniser_length + 5
end_of_ai = tokeniser_length + 6
pad_token = tokeniser_length + 7

audio_tokens_start = tokeniser_length + 10


def remove_duplicate_frames(example):
    try:
        answer_audio = example.get("audio")
        # If there's a valid audio array, tokenise it
        if answer_audio and "array" in answer_audio:
            audio_array = answer_audio["array"]
            codes_list = tokenise_audio(audio_array)
    except Exception as e:
        print(f"Skipping row due to error: {e}")
        # Keep codes_list as None if we fail
    vals = codes_list
    if len(vals) % 7 != 0:
        raise ValueError("Input list length must be divisible by 7")

    result = vals[:7]

    removed_frames = 0

    for i in range(7, len(vals), 7):
        current_first = vals[i]
        previous_first = result[-7]

        if current_first != previous_first:
            result.extend(vals[i : i + 7])
        else:
            removed_frames += 1

    example["codes_list"] = result

    return example


dataset = dataset.map(remove_duplicate_frames)

tok_info = """*** HERE you can modify the text prompt
If you are training a multi-speaker model (e.g., canopylabs/orpheus-3b-0.1-ft),
ensure that the dataset includes a "source" field and format the input accordingly:
- Single-speaker: f"{example['text']}"
- Multi-speaker: f"{example['source']}: {example['text']}"
"""
print(tok_info)


def create_input_ids(example):
    # Determine whether to include the source field
    text_prompt = (
        f"{example['source']}: {example['text']}"
        if "source" in example
        else example["text"]
    )

    text_ids = tokenizer.encode(text_prompt, add_special_tokens=True)
    text_ids.append(end_of_text)

    example["text_tokens"] = text_ids
    input_ids = (
        [start_of_human]
        + example["text_tokens"]
        + [end_of_human]
        + [start_of_ai]
        + [start_of_speech]
        + example["codes_list"]
        + [end_of_speech]
        + [end_of_ai]
    )
    example["labels"] = input_ids
    return example


dataset = dataset.map(create_input_ids, remove_columns=["codes_list","text_tokens"])
dataset = dataset.rename_column("text", "prompt")
dataset = dataset.map(
    lambda x: {
        "prompt": [{"role": "user", "content": f'<custom_token_3>{x["prompt"]}'}],
        "answer": "".join(
            tokenizer.batch_decode(x["labels"][x["labels"].index(128009) :])
        ).replace("<|eot_id|>", ""),
    }
)
# Save the dataset to disk
# dataset.save_to_disk("dataset")

dataset[0]["prompt"]

max_prompt_length = 2048 + 1  # + 1 just in case!
max_completion_length = 2048

from vllm import SamplingParams

vllm_sampling_params = SamplingParams(
    min_p=0.1,
    top_p=0.95,
    top_k=-1,
    seed=3407,
    stop=[tokenizer.eos_token],
    include_stop_str_in_output=True,
)

from trl import GRPOConfig, GRPOTrainer

training_args = GRPOConfig(
    vllm_sampling_params=vllm_sampling_params,
    temperature=0.8,
    learning_rate=5e-6,
    weight_decay=0.01,
    warmup_ratio=0.1,
    lr_scheduler_type="linear",
    optim="adamw_8bit",
    logging_steps=1,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=1,  # Increase to 4 for smoother training
    num_generations=4,  # Decrease if out of memory
    max_prompt_length=max_prompt_length,
    max_completion_length=max_completion_length,
    num_train_epochs=1,  # Set to 1 for a full training run
    save_steps=50,
    report_to="none",  # Can use Weights & Biases
    output_dir="outputs",
    epsilon=0.2,
    epsilon_high=0.28, # one sided
    delta=1.5, # two sided
    loss_type='bnpo',
    beta=0,
    mask_truncated_completions=True,
    # For optional training + evaluation
    # fp16_full_eval = True,
    # per_device_eval_batch_size = 4,
    # eval_accumulation_steps = 1,
    # eval_strategy = "steps",
    # eval_steps = 1,
)

from snac import SNAC
import wespeaker
from transformers import WhisperForConditionalGeneration, AutoProcessor
from transformers import pipeline
import torch
from evaluate import load
from audiobox_aesthetics.infer import initialize_predictor
import torchaudio.transforms as T
from torchmetrics.audio import PerceptualEvaluationSpeechQuality

whisper_model = WhisperForConditionalGeneration.from_pretrained(
    "openai/whisper-large-v3-turbo",
    device_map="cuda",
).cuda()
processor = AutoProcessor.from_pretrained(
    "openai/whisper-large-v3-turbo",
    language="English",
    task="transcribe",
)

whisper_model.eval()
# Create pipeline without specifying the device
whisper = pipeline(
    "automatic-speech-recognition",
    model=whisper_model,
    tokenizer=processor.tokenizer,
    feature_extractor=processor.feature_extractor,
    processor=processor,
    return_language=True,
    return_timestamps=True,
    device_map="cuda",
    torch_dtype=torch.bfloat16,
)

snac_model = SNAC.from_pretrained("hubertsiuzdak/snac_24khz")
snac_model = snac_model.cuda()


wespeaker_model = wespeaker.load_model("english")
wespeaker_model.set_device("cuda")


wer_eval = load("wer")

RESAMPLER_24_TO_16 = T.Resample(orig_freq=24000, new_freq=16000).to("cuda")
pesq_score = PerceptualEvaluationSpeechQuality(fs=16000, mode="wb").to("cuda")

predictor = initialize_predictor()

#Extra code
import re
import torch

def prepare_snac_codes(input_ids_chunk, device="cuda"):
    if isinstance(input_ids_chunk, torch.Tensor):
        if input_ids_chunk.ndim != 1:
            raise ValueError("If input_ids_chunk is a tensor, it must be 1D.")
        token_id_list = [item.item() for item in input_ids_chunk]
    elif isinstance(input_ids_chunk, list):
        if not all(isinstance(item, int) for item in input_ids_chunk):
            raise ValueError("input_ids_chunk must be a list of Python integers.")
        token_id_list = input_ids_chunk
    else:
        raise TypeError("input_ids_chunk must be a list of Python integers or a 1D PyTorch tensor.")

    list_length = len(token_id_list)

    # Each group of 7 tokens produces output. If less than 7, it will be empty.
    if list_length < 7:
        empty_long_tensor = torch.tensor([], dtype=torch.long, device=device).unsqueeze(0)
        return [empty_long_tensor.clone(), empty_long_tensor.clone(), empty_long_tensor.clone()]

    new_length = (list_length // 7) * 7
    if new_length == 0: # Should be caught by list_length < 7 already
        empty_long_tensor = torch.tensor([], dtype=torch.long, device=device).unsqueeze(0)
        return [empty_long_tensor.clone(), empty_long_tensor.clone(), empty_long_tensor.clone()]

    trimmed_list = token_id_list[:new_length]
    # Original token_ids for debugging if an issue occurs
    original_tokens_for_trimmed_list = token_id_list[:new_length]
    processed_ids = [token_id - 128266 for token_id in trimmed_list]

    layer_1_items = []
    layer_2_items = []
    layer_3_items = []
    num_groups = len(processed_ids) // 7

    VOCAB_SIZE_PER_QUANTIZER = 4096

    for i in range(num_groups):
        base_idx = 7 * i

        current_original_tokens = original_tokens_for_trimmed_list[base_idx : base_idx+7]
        current_processed_ids = processed_ids[base_idx : base_idx+7]

        def check_and_append(item_list, value, processed_val, original_token_val, layer_name, group_idx, pos_in_group):
            item_list.append(value)
            if not (0 <= value < VOCAB_SIZE_PER_QUANTIZER):
                print(f"!!! prepare_snac_codes: Out-of-range value for {layer_name}!")
                print(f"    Group {group_idx}, Pos {pos_in_group}: Calculated_Value={value}")
                print(f"    From Processed_ID: {processed_val} (Original Token ID: {original_token_val})")
                # Consider raising an error here or returning special value to prevent CUDA crash
                raise ValueError(f"Out-of-range SNAC code generated: {value}")

        # Layer 1 (pos 0)
        val = current_processed_ids[0]
        check_and_append(layer_1_items, val, current_processed_ids[0], current_original_tokens[0], "L1_P0", i, 0)

        # Layer 2 (pos 1)
        val = current_processed_ids[1] - (1 * VOCAB_SIZE_PER_QUANTIZER)
        check_and_append(layer_2_items, val, current_processed_ids[1], current_original_tokens[1], "L2_P1", i, 1)

        # Layer 3 (pos 2)
        val = current_processed_ids[2] - (2 * VOCAB_SIZE_PER_QUANTIZER)
        check_and_append(layer_3_items, val, current_processed_ids[2], current_original_tokens[2], "L3_P2", i, 2)

        # Layer 3 (pos 3)
        val = current_processed_ids[3] - (3 * VOCAB_SIZE_PER_QUANTIZER)
        check_and_append(layer_3_items, val, current_processed_ids[3], current_original_tokens[3], "L3_P3", i, 3)

        # Layer 2 (pos 4)
        val = current_processed_ids[4] - (4 * VOCAB_SIZE_PER_QUANTIZER)
        check_and_append(layer_2_items, val, current_processed_ids[4], current_original_tokens[4], "L2_P4", i, 4)

        # Layer 3 (pos 5)
        val = current_processed_ids[5] - (5 * VOCAB_SIZE_PER_QUANTIZER)
        check_and_append(layer_3_items, val, current_processed_ids[5], current_original_tokens[5], "L3_P5", i, 5)

        # Layer 3 (pos 6)
        val = current_processed_ids[6] - (6 * VOCAB_SIZE_PER_QUANTIZER)
        check_and_append(layer_3_items, val, current_processed_ids[6], current_original_tokens[6], "L3_P6", i, 6)

    codes = [
        torch.tensor(layer_1_items, dtype=torch.long, device=device).unsqueeze(0),
        torch.tensor(layer_2_items, dtype=torch.long, device=device).unsqueeze(0),
        torch.tensor(layer_3_items, dtype=torch.long, device=device).unsqueeze(0)
    ]
    return codes
def decode_in_chunks(snac_model_instance, all_token_ids, chunk_size_tokens, device="cuda"):
    audio_segments = []

    if isinstance(all_token_ids, torch.Tensor):
        all_token_ids_list = all_token_ids.cpu().tolist()
    elif isinstance(all_token_ids, list):
        all_token_ids_list = all_token_ids
    else:
        raise TypeError("all_token_ids must be a list of Python integers or a 1D PyTorch tensor.")

    if not all_token_ids_list:
        print("Warning: decode_in_chunks received an empty list of token IDs.")
        return torch.tensor([[]], dtype=torch.float32, device='cpu') # Return empty audio on CPU

    if chunk_size_tokens % 7 != 0:
        print(f"Warning: chunk_size_tokens ({chunk_size_tokens}) is not a multiple of 7. "
              "Some tokens at the end of chunks might be discarded by prepare_snac_codes. "
              "Consider using a multiple of 7 for chunk_size_tokens.")

    num_total_tokens = len(all_token_ids_list)
    # print(f"Total tokens to process: {num_total_tokens}")

    for i in range(0, num_total_tokens, chunk_size_tokens):
        token_chunk = all_token_ids_list[i : i + chunk_size_tokens]

        if not token_chunk:
            continue


        codes_chunk = prepare_snac_codes(token_chunk, device=device)

        if codes_chunk[0].numel() == 0:
            del codes_chunk
            if torch.cuda.is_available() and 'cuda' in device:
                 torch.cuda.empty_cache()
            continue

        audio_hat_chunk = None # Initialize to ensure it's defined for del
        try:
            with torch.no_grad():
                audio_hat_chunk_decoded = snac_model_instance.decode(codes_chunk)
                if audio_hat_chunk_decoded.ndim == 2: # If it's [B, L]
                    audio_hat_chunk_decoded = audio_hat_chunk_decoded.unsqueeze(1) # Make it [B, 1, L]

                audio_segments.append(audio_hat_chunk_decoded.cpu())
        except Exception as e:
            print(f"Error during snac_model.decode for chunk starting at token index {i}: {e}")

        del codes_chunk
        if audio_hat_chunk is not None: # Check if it was assigned in try block
             del audio_hat_chunk # This was from before, it should be audio_hat_chunk_decoded
        if 'audio_hat_chunk_decoded' in locals() and audio_hat_chunk_decoded is not None:
            del audio_hat_chunk_decoded # Delete the tensor that was on GPU

        if torch.cuda.is_available() and 'cuda' in device:
            torch.cuda.empty_cache()
        # print(f"VRAM allocated on {device} after chunk {i // chunk_size_tokens + 1}: {torch.cuda.memory_allocated(device) / (1024**2):.2f} MB")


    if not audio_segments:
        print("Warning: No audio segments were generated. Input might have been too short or all chunks resulted in empty codes.")
        return torch.tensor([[]], dtype=torch.float32, device='cpu')

    try:
        # All segments in audio_segments should now be [B, C, L_i] (likely [1, 1, L_i])
        full_audio = torch.cat(audio_segments, dim=2) # CONCATENATE ALONG SEQUENCE DIMENSION
    except Exception as e:
        print(f"Error concatenating audio segments: {e}")
        print(f"Number of segments: {len(audio_segments)}")
        for idx, seg in enumerate(audio_segments):
            print(f"Segment {idx} shape: {seg.shape}, dtype: {seg.dtype}")
        raise e

    return full_audio

def align_tensors(pred_tensor, gt_tensor, mode='trim'):
    len_pred = pred_tensor.shape[-1]
    len_gt = gt_tensor.shape[-1]

    if len_pred == len_gt:
        return pred_tensor, gt_tensor

    if mode == 'trim':
        min_len = min(len_pred, len_gt)
        return pred_tensor[..., :min_len], gt_tensor[..., :min_len]

    elif mode == 'pad':
        max_len = max(len_pred, len_gt)
        # Pad the shorter audio to match the max length
        if len_pred < max_len:
            pad_width = (0, max_len - len_pred) # (pad_start, pad_end)
            pred_tensor = torch.nn.functional.pad(pred_tensor, pad_width, "constant", 0)
        else: # len_gt < max_len
            pad_width = (0, max_len - len_gt)
            gt_tensor = torch.nn.functional.pad(gt_tensor, pad_width, "constant", 0)
        return pred_tensor, gt_tensor

    else:
        raise ValueError(f"Unsupported alignment_mode: '{mode}'. Choose 'trim' or 'pad'.")

GLOBAL_RAW = []
#Aesthetics works very well with english , you can use dnsmos for other languages
def aesthetics_reward(prompts, completions, **kwargs):
    scores = []
    try:
        predictors = predictor.forward([
        {
            "path": wav_aligned.detach().squeeze().unsqueeze(0),
            "sample_rate": 24000
        }
        for wav_aligned in GLOBAL_RAW if wav_aligned is not -1
        ])
    except Exception as e:
        print(f"Aesthetics Error: {e}")
        return [-0.1] * training_args.num_generations
    for i in predictors:
        score = i['PQ']
        if score > 7 :
            score -= 7
            if score >1.6:
                scores.append(score+0.15)
            else:
                scores.append(score)
        else:
            scores.append(-1)
    if len(scores) != training_args.num_generations:
        print(f"old scores : {len(scores)}")
        scores += [0] * (training_args.num_generations - len(scores))
        print(f"new scores : {len(scores)}")
    return scores
def pesq_reward(prompts, completions, **kwargs):
    scores = []
    try:
        gt_codes = tokenizer.encode(kwargs['answer'][0].replace("<custom_token_4><custom_token_5><custom_token_1>",'').replace("<custom_token_2><custom_token_6><custom_token_3>",''))[1:]
        # Decode to raw audio at the model's native sample rate (e.g., 24kHz)
        gt_raw_audio = decode_in_chunks(snac_model, gt_codes, 7 * 100, "cuda")
        # RESAMPLE to 16kHz for the PESQ metric
        gt_resampled_audio = RESAMPLER_24_TO_16(gt_raw_audio.detach().squeeze().unsqueeze(0).to("cuda"))
    except Exception as e:
        print(f"PESQ Error: Could not decode or resample ground truth audio. Returning error for batch. Details: {e}")
        # If we can't get a valid ground truth, no scores can be calculated
        return [0] * training_args.num_generations
    for codes in GLOBAL_RAW:
        if codes is -1:
            scores.append(-1)
            continue
        try:
            pred_resampled_audio = RESAMPLER_24_TO_16(codes.detach().squeeze().unsqueeze(0).to("cuda"))

            # Align the tensors to ensure they have the same length
            gt_resampled_audio, pred_resampled_audio = align_tensors(gt_resampled_audio, pred_resampled_audio, mode='pad')

            score = pesq_score(pred_resampled_audio, gt_resampled_audio).item()
            scores.append(score)
        except Exception as e:
            print(f"PESQ Error: {e}")
            scores.append(-1)

    return scores
def speaker_similarity(prompts, completions, **kwargs):
    ground_trauth_codes = tokenizer.encode(kwargs['answer'][0].replace("<custom_token_4><custom_token_5><custom_token_1>",'').replace("<custom_token_2><custom_token_6><custom_token_3>",''))[1:]
    if ground_trauth_codes == []:
        return [0] * training_args.num_generations
    ground_trauth_raw = decode_in_chunks(snac_model,ground_trauth_codes,7*100,"cuda")
    ground_trauth_embeds = wespeaker_model.extract_embedding_from_pcm(ground_trauth_raw.detach().squeeze().unsqueeze(0), 24000).cpu()
    scores = []
    for codes in GLOBAL_RAW:
        if codes is -1:
            scores.append(-1)
            continue
        try:
            predictions_embeds = wespeaker_model.extract_embedding_from_pcm(codes.detach().squeeze().unsqueeze(0), 24000).cpu()
            similarity_score = wespeaker_model.cosine_similarity(ground_trauth_embeds, predictions_embeds)
            if similarity_score < 0.7 :
                scores.append(similarity_score - 0.2)
            elif similarity_score >= 0.9:
                scores.append(2)
            else:
                scores.append(similarity_score + 0.5)
        except Exception as e:
            print("Error speaker_similarity: ",e)
            scores.append(-1)

    GLOBAL_RAW.clear()

    return scores
def audio_length(prompts, completions, **kwargs):
    scores = []
    pattern = r"<custom_token_\d+>"
    try:
        ground_trauth_length = len(tokenizer.encode(kwargs['answer'][0].replace("<custom_token_4><custom_token_5><custom_token_1>",'').replace("<custom_token_2><custom_token_6><custom_token_3>",''))[1:])
        for codes in completions:
            codes_length = len(tokenizer.encode(''.join(re.findall(pattern,codes[0]['content'])).replace("<custom_token_4><custom_token_5><custom_token_1>",'').replace("<custom_token_2><custom_token_6><custom_token_3>",''))[1:])
            if ground_trauth_length == codes_length or ground_trauth_length - 87*2 < codes_length < ground_trauth_length + 87*2 :
                scores.append(0.5)
            else:
                scores.append(-0.5)
    except Exception as e:
        print("Error audio_length: ",e)
        scores.append(0)
    if len(scores) != training_args.num_generations:
        print(f"old scores audio_length : {len(scores)}")
        scores += [0] * (training_args.num_generations - len(scores))
        print(f"new scores : {len(scores)}")
    return scores
def wer(prompts, completions, **kwargs):
    GLOBAL_RAW.clear()
    prompt = prompts[0][-1]["content"]
    #removing emotive tags <*>
    cleaned_text = re.sub(r"<[^>]*>", "", prompt)

    pattern = r"<custom_token_\d+>"
    scores = []
    for codes in completions:
        codes = tokenizer.encode(''.join(re.findall(pattern,codes[0]['content'])).replace("<custom_token_4><custom_token_5><custom_token_1>",'').replace("<custom_token_2><custom_token_6><custom_token_3>",''))[1:]
        if codes == []:
            scores.append(0)
            GLOBAL_RAW.append(-1)
            continue
        try:
            snac_output = decode_in_chunks(snac_model,codes,7 * 100,"cuda")
            asr_outputs = whisper(snac_output.detach().squeeze().cpu().numpy())
            output_texts = [i['text'] for i in asr_outputs['chunks']]
            wer_score = wer_eval.compute(predictions=[output_texts[0]], references=[cleaned_text])
            score = max(-1, 1.0 - wer_score)
            GLOBAL_RAW.append(snac_output)
            scores.append(score)
        except Exception as e:
            GLOBAL_RAW.append(-1)
            print("Error wer: ",e)
            scores.append(-1)

    return scores

tokenizer.chat_template = """
{% for message in messages %}
{{ message['content'] }}
{% endfor %}
"""

dataset[3]["prompt"]

# For optional training + evaluation
# new_dataset = dataset.train_test_split(test_size = 0.01)

trainer = GRPOTrainer(
    model=model,
    processing_class=tokenizer,
    reward_funcs=[wer, audio_length ,pesq_reward ,aesthetics_reward ,speaker_similarity],
    args=training_args,
    train_dataset=dataset,
    # For optional training + evaluation
    # train_dataset = new_dataset["train"],
    # eval_dataset = new_dataset["test"],
)

trainer.train(resume_from_checkpoint = True)

model.save_pretrained("lora_model")  # Local saving
tokenizer.save_pretrained("lora_model")
# model.push_to_hub("your_name/lora_model", token = "...") # Online saving
# tokenizer.push_to_hub("your_name/lora_model", token = "...") # Online saving

